{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854e0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ml.models import MLPClassifier\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85b15a",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a747a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metadata(model):\n",
    "    params = 0\n",
    "    \n",
    "    message = f'''\n",
    "    ============================================================================================\n",
    "    Layers size: {tuple(model.layers_size)}\n",
    "    ============================================================================================\n",
    "    Activations: {tuple(model.activations)}'''\n",
    "    \n",
    "    for key, value in model.parameters.items():\n",
    "        params += value.shape[0] * value.shape[1]\n",
    "        \n",
    "    message = f'''{message}\n",
    "    ============================================================================================\n",
    "    Trainable parameters: {params}\n",
    "    ============================================================================================\n",
    "    '''\n",
    "    \n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27510a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse=False).fit(np.array([i for i in range(10)]).reshape(-1, 1))\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2)\n",
    "\n",
    "X_train, X_test, y_train = X_train.T, X_test.T, encoder.transform(y_train.reshape(-1, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd44f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ============================================================================================\n",
      "    Layers size: (64, 32, 16, 10)\n",
      "    ============================================================================================\n",
      "    Activations: ('tanh', 'tanh', 'softmax')\n",
      "    ============================================================================================\n",
      "    Trainable parameters: 2778\n",
      "    ============================================================================================\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(X_train.shape[0], 10, hidden_layers_size=(32,16), activations=('tanh','tanh'))\n",
    "print_metadata(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ca75c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4601948726097214\n",
      "Loss: 2.4499773389717627\n",
      "Loss: 2.4398487588968805\n",
      "Loss: 2.4298085837724654\n",
      "Loss: 2.4198562458475017\n",
      "Loss: 2.4099911605985427\n",
      "Loss: 2.4002127289377384\n",
      "Loss: 2.39052033925895\n",
      "Loss: 2.3809133693202744\n",
      "Loss: 2.3713911879633343\n",
      "Loss: 2.361953156671486\n",
      "Loss: 2.3525986309706557\n",
      "Loss: 2.3433269616778443\n",
      "Loss: 2.334137496003459\n",
      "Loss: 2.3250295785145445\n",
      "Loss: 2.3160025519667355\n",
      "Loss: 2.3070557580132864\n",
      "Loss: 2.298188537799983\n",
      "Loss: 2.2894002324549696\n",
      "Loss: 2.2806901834826743\n",
      "Loss: 2.272057733071065\n",
      "Loss: 2.263502224321338\n",
      "Loss: 2.2550230014090293\n",
      "Loss: 2.246619409685247\n",
      "Loss: 2.2382907957264377\n",
      "Loss: 2.2300365073407082\n",
      "Loss: 2.221855893538321\n",
      "Loss: 2.2137483044735013\n",
      "Loss: 2.20571309136423\n",
      "Loss: 2.1977496063961555\n",
      "Loss: 2.1898572026162513\n",
      "Loss: 2.1820352338213085\n",
      "Loss: 2.1742830544457967\n",
      "Loss: 2.1666000194531225\n",
      "Loss: 2.1589854842337663\n",
      "Loss: 2.151438804513284\n",
      "Loss: 2.1439593362726725\n",
      "Loss: 2.1365464356831168\n",
      "Loss: 2.1291994590567187\n",
      "Loss: 2.1219177628143697\n",
      "Loss: 2.1147007034715744\n",
      "Loss: 2.1075476376426603\n",
      "Loss: 2.1004579220635127\n",
      "Loss: 2.09343091363267\n",
      "Loss: 2.0864659694703875\n",
      "Loss: 2.079562446995043\n",
      "Loss: 2.0727197040160776\n",
      "Loss: 2.065937098842522\n",
      "Loss: 2.0592139904060085\n",
      "Loss: 2.052549738397097\n",
      "Loss: 2.045943703413658\n",
      "Loss: 2.0393952471199914\n",
      "Loss: 2.0329037324153765\n",
      "Loss: 2.02646852361067\n",
      "Loss: 2.020088986611648\n",
      "Loss: 2.013764489107759\n",
      "Loss: 2.007494400765007\n",
      "Loss: 2.001278093421732\n",
      "Loss: 1.9951149412861013\n",
      "Loss: 1.9890043211341921\n",
      "Loss: 1.9829456125076006\n",
      "Loss: 1.9769381979096001\n",
      "Loss: 1.970981462998926\n",
      "Loss: 1.9650747967803526\n",
      "Loss: 1.9592175917912946\n",
      "Loss: 1.9534092442837412\n",
      "Loss: 1.9476491544009105\n",
      "Loss: 1.941936726348074\n",
      "Loss: 1.9362713685570832\n",
      "Loss: 1.9306524938441767\n",
      "Loss: 1.9250795195607455\n",
      "Loss: 1.9195518677367496\n",
      "Loss: 1.9140689652165812\n",
      "Loss: 1.908630243787186\n",
      "Loss: 1.90323514029834\n",
      "Loss: 1.8978830967749873\n",
      "Loss: 1.8925735605216305\n",
      "Loss: 1.8873059842187627\n",
      "Loss: 1.882079826011401\n",
      "Loss: 1.876894549589795\n",
      "Loss: 1.8717496242624114\n",
      "Loss: 1.866644525021347\n",
      "Loss: 1.8615787326003055\n",
      "Loss: 1.8565517335253354\n",
      "Loss: 1.8515630201585136\n",
      "Loss: 1.8466120907347952\n",
      "Loss: 1.8416984493922424\n",
      "Loss: 1.836821606195883\n",
      "Loss: 1.8319810771554288\n",
      "Loss: 1.8271763842371098\n",
      "Loss: 1.8224070553698783\n",
      "Loss: 1.817672624446246\n",
      "Loss: 1.8129726313180015\n",
      "Loss: 1.8083066217870805\n",
      "Loss: 1.8036741475918399\n",
      "Loss: 1.7990747663889959\n",
      "Loss: 1.7945080417314745\n",
      "Loss: 1.7899735430424297\n",
      "Loss: 1.7854708455856656\n",
      "Loss: 1.7809995304327098\n",
      "Loss: 1.7765591844267579\n",
      "Loss: 1.772149400143728\n",
      "Loss: 1.7677697758506297\n",
      "Loss: 1.7634199154614691\n",
      "Loss: 1.7590994284908823\n",
      "Loss: 1.7548079300057031\n",
      "Loss: 1.7505450405746377\n",
      "Loss: 1.7463103862162384\n",
      "Loss: 1.7421035983453343\n",
      "Loss: 1.7379243137180884\n",
      "Loss: 1.7337721743758323\n",
      "Loss: 1.7296468275878214\n",
      "Loss: 1.7255479257930524\n",
      "Loss: 1.7214751265412689\n",
      "Loss: 1.71742809243328\n",
      "Loss: 1.713406491060705\n",
      "Loss: 1.7094099949452526\n",
      "Loss: 1.7054382814776372\n",
      "Loss: 1.7014910328562198\n",
      "Loss: 1.6975679360254716\n",
      "Loss: 1.6936686826143308\n",
      "Loss: 1.6897929688745397\n",
      "Loss: 1.6859404956190192\n",
      "Loss: 1.6821109681603585\n",
      "Loss: 1.6783040962494735\n",
      "Loss: 1.6745195940144852\n",
      "Loss: 1.6707571798998764\n",
      "Loss: 1.6670165766059706\n",
      "Loss: 1.6632975110287695\n",
      "Loss: 1.6595997142001975\n",
      "Loss: 1.6559229212287772\n",
      "Loss: 1.6522668712407784\n",
      "Loss: 1.6486313073218595\n",
      "Loss: 1.645015976459232\n",
      "Loss: 1.6414206294843723\n",
      "Loss: 1.6378450210162923\n",
      "Loss: 1.6342889094053996\n",
      "Loss: 1.6307520566779508\n",
      "Loss: 1.627234228481121\n",
      "Loss: 1.6237351940286926\n",
      "Loss: 1.6202547260473816\n",
      "Loss: 1.6167926007238047\n",
      "Loss: 1.6133485976520934\n",
      "Loss: 1.609922499782165\n",
      "Loss: 1.6065140933686524\n",
      "Loss: 1.6031231679204918\n",
      "Loss: 1.599749516151179\n",
      "Loss: 1.596392933929686\n",
      "Loss: 1.5930532202320455\n",
      "Loss: 1.5897301770935974\n",
      "Loss: 1.5864236095618982\n",
      "Loss: 1.583133325650291\n",
      "Loss: 1.5798591362921344\n",
      "Loss: 1.576600855295681\n",
      "Loss: 1.573358299299613\n",
      "Loss: 1.5701312877292186\n",
      "Loss: 1.5669196427532113\n",
      "Loss: 1.5637231892411867\n",
      "Loss: 1.5605417547217073\n",
      "Loss: 1.5573751693410165\n",
      "Loss: 1.5542232658223663\n",
      "Loss: 1.551085879425963\n",
      "Loss: 1.5479628479095167\n",
      "Loss: 1.5448540114893885\n",
      "Loss: 1.541759212802336\n",
      "Loss: 1.5386782968678399\n",
      "Loss: 1.5356111110510116\n",
      "Loss: 1.5325575050260714\n",
      "Loss: 1.5295173307403944\n",
      "Loss: 1.526490442379109\n",
      "Loss: 1.523476696330249\n",
      "Loss: 1.5204759511504464\n",
      "Loss: 1.5174880675311604\n",
      "Loss: 1.5145129082654314\n",
      "Loss: 1.51155033821516\n",
      "Loss: 1.508600224278894\n",
      "Loss: 1.5056624353601231\n",
      "Loss: 1.5027368423360705\n",
      "Loss: 1.4998233180269758\n",
      "Loss: 1.4969217371658623\n",
      "Loss: 1.4940319763687757\n",
      "Loss: 1.4911539141054966\n",
      "Loss: 1.4882874306707095\n",
      "Loss: 1.4854324081556318\n",
      "Loss: 1.4825887304200827\n",
      "Loss: 1.4797562830649982\n",
      "Loss: 1.476934953405375\n",
      "Loss: 1.4741246304436442\n",
      "Loss: 1.4713252048434589\n",
      "Loss: 1.4685365689039003\n",
      "Loss: 1.4657586165340841\n",
      "Loss: 1.4629912432281706\n",
      "Loss: 1.460234346040763\n",
      "Loss: 1.4574878235626951\n",
      "Loss: 1.4547515758971965\n",
      "Loss: 1.4520255046364337\n",
      "Loss: 1.4493095128384132\n",
      "Loss: 1.4466035050042543\n",
      "Loss: 1.4439073870558055\n",
      "Loss: 1.441221066313621\n",
      "Loss: 1.4385444514752723\n",
      "Loss: 1.4358774525940017\n",
      "Loss: 1.433219981057705\n",
      "Loss: 1.4305719495682419\n",
      "Loss: 1.4279332721210651\n",
      "Loss: 1.4253038639851665\n",
      "Loss: 1.4226836416833328\n",
      "Loss: 1.4200725229727027\n",
      "Loss: 1.417470426825629\n",
      "Loss: 1.4148772734108301\n",
      "Loss: 1.412292984074832\n",
      "Loss: 1.409717481323697\n",
      "Loss: 1.40715068880503\n",
      "Loss: 1.4045925312902592\n",
      "Loss: 1.402042934657189\n",
      "Loss: 1.3995018258728176\n",
      "Loss: 1.3969691329764151\n",
      "Loss: 1.3944447850628607\n",
      "Loss: 1.3919287122662307\n",
      "Loss: 1.3894208457436354\n",
      "Loss: 1.3869211176593002\n",
      "Loss: 1.3844294611688879\n",
      "Loss: 1.3819458104040558\n",
      "Loss: 1.379470100457246\n",
      "Loss: 1.3770022673667053\n",
      "Loss: 1.3745422481017269\n",
      "Loss: 1.3720899805481173\n",
      "Loss: 1.3696454034938776\n",
      "Loss: 1.3672084566151006\n",
      "Loss: 1.3647790804620779\n",
      "Loss: 1.3623572164456141\n",
      "Loss: 1.3599428068235466\n",
      "Loss: 1.3575357946874633\n",
      "Loss: 1.355136123949621\n",
      "Loss: 1.352743739330055\n",
      "Loss: 1.3503585863438838\n",
      "Loss: 1.347980611288799\n",
      "Loss: 1.3456097612327433\n",
      "Loss: 1.3432459840017694\n",
      "Loss: 1.340889228168079\n",
      "Loss: 1.3385394430382407\n",
      "Loss: 1.3361965786415806\n",
      "Loss: 1.3338605857187442\n",
      "Loss: 1.3315314157104319\n",
      "Loss: 1.3292090207462937\n",
      "Loss: 1.3268933536339966\n",
      "Loss: 1.3245843678484475\n",
      "Loss: 1.3222820175211796\n",
      "Loss: 1.3199862574298928\n",
      "Loss: 1.3176970429881527\n",
      "Loss: 1.3154143302352388\n",
      "Loss: 1.3131380758261446\n",
      "Loss: 1.3108682370217277\n",
      "Loss: 1.308604771679004\n",
      "Loss: 1.3063476382415853\n",
      "Loss: 1.3040967957302634\n",
      "Loss: 1.3018522037337297\n",
      "Loss: 1.2996138223994347\n",
      "Loss: 1.2973816124245858\n",
      "Loss: 1.295155535047277\n",
      "Loss: 1.2929355520377515\n",
      "Loss: 1.2907216256897978\n",
      "Loss: 1.2885137188122706\n",
      "Loss: 1.2863117947207432\n",
      "Loss: 1.2841158172292835\n",
      "Loss: 1.2819257506423525\n",
      "Loss: 1.279741559746829\n",
      "Loss: 1.2775632098041512\n",
      "Loss: 1.2753906665425792\n",
      "Loss: 1.2732238961495757\n",
      "Loss: 1.2710628652642997\n",
      "Loss: 1.2689075409702189\n",
      "Loss: 1.2667578907878305\n",
      "Loss: 1.264613882667498\n",
      "Loss: 1.2624754849823936\n",
      "Loss: 1.260342666521553\n",
      "Loss: 1.2582153964830336\n",
      "Loss: 1.2560936444671844\n",
      "Loss: 1.2539773804700127\n",
      "Loss: 1.251866574876659\n",
      "Loss: 1.2497611984549735\n",
      "Loss: 1.2476612223491903\n",
      "Loss: 1.2455666180737035\n",
      "Loss: 1.2434773575069393\n",
      "Loss: 1.2413934128853263\n",
      "Loss: 1.2393147567973586\n",
      "Loss: 1.2372413621777574\n",
      "Loss: 1.2351732023017201\n",
      "Loss: 1.233110250779265\n",
      "Loss: 1.2310524815496666\n",
      "Loss: 1.2289998688759765\n",
      "Loss: 1.2269523873396369\n",
      "Loss: 1.224910011835179\n",
      "Loss: 1.2228727175650067\n",
      "Loss: 1.220840480034267\n",
      "Loss: 1.2188132750458018\n",
      "Loss: 1.2167910786951852\n",
      "Loss: 1.2147738673658388\n",
      "Loss: 1.2127616177242304\n",
      "Loss: 1.2107543067151507\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bdf318",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c7733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
